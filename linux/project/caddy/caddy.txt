latitude=192.168.2.200 - wrk
amd=201 - wkr
asus=202 -  wkr
aceri3=203 - mgr
aceri5=192.168.2.205 - mgr aceri5.lln3nc.ca

https://caddyserver.com/

There are no ports exposed here in the front-end section. In fact, the only exposed ports are for mail, hog and Mongo and Postgres, and that's not going to help us hit our web application. 
That's a simple web server, a web server that will take requests and forward them to the appropriate microservice. 
In other words, a reverse proxy. There's lots of web servers to choose from. We could go with Apache or Engine X, and if you're comfortable with those, you may want to use that. 
But I'm going to use this one. Let me switch to my web browser. I'm going to use Caddy, it is a really good web server. It's written in GO. 
And one of the nice things about it is it handles installation and deployment of SSL certificates automatically. We don't have to do anything if we have an appropriate domain name. 
We've registered a domain name, and we have the appropriate entry in the name servers. 
It just magically puts all the SSL certificates in place for us and it uses let's encrypt, which is extremely popular and doesn't cost anything. 
"Most people use Caddy as a web server or proxy, but at its core, Caddy is a server of servers. With the requisite modules, it can take on the role of any long-running process!"


__________________________________________________________________________________________________________________________________________________________________________________
write caddyfile for local network nodes used for docker swarm cluster

Below is an example Caddyfile for managing and reverse proxying a Docker Swarm cluster on a local network. This assumes you have multiple services running on your nodes and want to route traffic based on subdomains or paths.

Example Caddyfile:

# Global options block
{
    auto_https off # Disable HTTPS since this is a local network
    admin 0.0.0.0:2019 # Enable remote admin interface
}

# Node 1 - Manager Node
manager.local.lan {
    reverse_proxy localhost:2377 # Docker Swarm API
}

# Node 2 - Worker Node 1
worker1.local.lan {
    reverse_proxy 192.168.1.101:8080 # Replace with the service port on worker 1
}

# Node 3 - Worker Node 2
worker2.local.lan {
    reverse_proxy 192.168.1.102:8080 # Replace with the service port on worker 2
}

# Route based on paths for services in the Swarm
services.l ocal.lan {
    reverse_proxy /service1* 192.168.1.101:8080
    reverse_proxy /service2* 192.168.1.102:8080
}

# Docker Swarm Visualizer (if used)
visualizer.local.lan {
    reverse_proxy 192.168.1.100:8080 # Replace with the visualizer service node and port
}

# General catch-all to manager node
*.local.lan {
    reverse_proxy localhost:80
}


Key Notes:
Hostnames: Ensure manager.local.lan, worker1.local.lan, and worker2.local.lan are resolvable on your local network. Update /etc/hosts or use a local DNS server if necessary.
Ports: Replace 8080, 2377, and other ports with the actual service ports exposed on your nodes.
Docker Swarm Integration: This configuration assumes services are properly deployed and running in the Swarm cluster. You may adjust the reverse_proxy destinations based on your Docker service constraints or replicas.
Disable HTTPS: Since this is for a local network, auto_https off disables automatic HTTPS and uses HTTP only.


__________________________________________________________________________________________________________________________________________________________________________________
write caddyfile for local network nodes


Use Case
You have multiple nodes on a local network with specific IPs or hostnames.
Caddy will act as a reverse proxy to direct requests based on hostnames or paths.
{
    # Global options
    admin off
    log {
        output file /var/log/caddy/access.log
        level info
    }
}

# Node 1: Accessible at http://node1.local
node1.local {
    reverse_proxy 192.168.1.101:80
}

# Node 2: Accessible at http://node2.local
node2.local {
    reverse_proxy 192.168.1.102:80
}

# Node 3: Path-based proxying
node3.local {
    route /app1/* {
        reverse_proxy 192.168.1.103:8080
    }
    route /app2/* {
        reverse_proxy 192.168.1.103:9090
    }
}

# Node 4: HTTPS-enabled with self-signed certs
node4.local {
    tls /etc/caddy/certs/node4.crt /etc/caddy/certs/node4.key
    reverse_proxy 192.168.1.104:443
}

# Catch-all fallback
:80 {
    respond "Unknown host. Check your configuration." 404
}

Explanation
Global Configurations:

Admin interface disabled.
Access logs stored at /var/log/caddy/access.log.
Per Node Config:

Each node is mapped to a hostname (e.g., node1.local).
Reverse proxy targets are specified by the node's local IP and port.
Path-Based Proxying:

Routes traffic based on URL paths (e.g., /app1, /app2).
HTTPS with TLS:

tls directive specifies custom certificates for HTTPS nodes.
Fallback:

A catch-all site serves a 404 response for unrecognized requests.
Local Network Setup
Update your local hosts file (e.g., /etc/hosts on Linux) to map node1.local, node2.local, etc., to the respective IPs:

192.168.1.101 node1.local
192.168.1.102 node2.local
192.168.1.103 node3.local
192.168.1.104 node4.local

Restart Caddy after saving the configuration:

caddy reload


SSL
__________________________________________________________________________________________________________________________________________________________________________________

Next step is to modify things so that our front end web server caddie listens on Port 443 with SSL certificates. 
And of course, one of the great things about caddy is it goes and gets those certificates for us. 
It updates them when they need to be updated and all that sort of thing. So I'm looking at the files swarm.production, I'm already listening to Port 443 and I already have mounted volumes to store the SSL certificates, which is caddy_data and caddy_config. 
If I look over on my terminal on node one inside the swarm folder, I have those two folders. 
To start with, we're going to want to update the front end here, this microservice, because that'll be going to an https connection -so we'll just add an s there (to http).  
We look at the file caddy file production on this file right here, this is the one that's copied into that Docker image.
And to make this work, I'm going to delete Colon 80 for my first virtual host. And now that it's going to be using SSL, I'm going to “import security” as well this bit, which just makes sure that things work well. 
There’s nothing that needs to change. And then of course for the broker service. And this tells Caddy, hey, we're going to be using auto generated and auto managed SSL certificates. My swarm deployment file now is swarmed up.
